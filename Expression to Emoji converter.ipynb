{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Imports\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "import cv2\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import threading\n",
    "\n",
    "\n",
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))\n",
    "emotion_model.load_weights('model.h5')\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "\n",
    "#  Using openCV haarcascade xml detect the bounding boxes of face in the webcam and predict the emotions:\n",
    "\n",
    "\n",
    "emotion_dict = {0: \"Sad\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Angry\", 6: \"Surprised\"}\n",
    "\n",
    "emoji_dist={0:\"C:\\\\Users\\\\Sourav\\\\Downloads\\\\Emojify-master\\\\Emojify-master\\\\emojis\\\\sad.png\",1:\"C:\\\\Users\\\\Sourav\\\\Downloads\\\\Emojify-master\\\\Emojify-master\\\\emojis\\\\disgust.png\",2:\"C:\\\\Users\\\\Sourav\\\\Downloads\\\\Emojify-master\\\\Emojify-master\\\\emojis\\\\fear.png\",3:\"C:\\\\Users\\\\Sourav\\\\Downloads\\\\Emojify-master\\\\Emojify-master\\\\emojis\\\\happy.png\",4:\"C:\\\\Users\\\\Sourav\\\\Downloads\\\\Emojify-master\\\\Emojify-master\\\\emojis\\\\neutral.png\",5:\"C:\\\\Users\\\\Sourav\\\\Downloads\\\\Emojify-master\\\\Emojify-master\\\\emojis\\\\angry.png\",6:\"C:\\\\Users\\\\Sourav\\\\Downloads\\\\Emojify-master\\\\Emojify-master\\\\emojis\\\\surprised.png\"}\n",
    "\n",
    "global last_frame1\n",
    "last1_frame1 = np.zeros((448,640,3), dtype = np.uint8)\n",
    "global cap1\n",
    "show_text = [0]\n",
    "global frame_number\n",
    "\n",
    "\n",
    "#  Using openCV haarcascade xml to detect the bounding boxes of face in the webcam and predict the emotions:\n",
    "\n",
    "def show_subject():\n",
    "    cap1 = cv2.VideoCapture(0)\n",
    "    if not cap1.isOpened():\n",
    "        print(\"Can't open the camera\")\n",
    "    global frame_number\n",
    "    length = int(cap1.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_number += 1\n",
    "    if frame_number >= length:\n",
    "        exit()\n",
    "    cap1.set(1, frame_number)\n",
    "    flag1, frame1 = cap1.read()\n",
    "    frame1 = cv2.resize(frame1,(600,500))\n",
    "    bounding_box = cv2.CascadeClassifier(\"C:\\\\Users\\\\Sourav\\\\Downloads\\\\Emojify-master\\\\Emojify-master\\\\opencv-master\\\\opencv-master\\\\data\\\\haarcascades\\\\haarcascade_frontalface_default.xml\")\n",
    "    gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    num_faces = bounding_box.detectMultiScale(gray_frame, scaleFactor = 1.3, minNeighbors = 5)\n",
    "    for (x,y,w,h) in num_faces:\n",
    "        cv2.rectangle(frame1, (x,y-50), (x+w, y+h+10), (255,0,0), 2)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48,48)), -1), 0)\n",
    "        prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(prediction))\n",
    "        cv2.putText(frame1, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "        show_text[0] = maxindex\n",
    "        \n",
    "    if flag1 is None:\n",
    "        print(\"Major error!\")\n",
    "    elif flag1:\n",
    "        global last_frame1\n",
    "        last_frame1 = frame1.copy()\n",
    "        pic = cv2.cvtColor(last_frame1,cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(pic)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        lmain.imgtk = imgtk\n",
    "        lmain.configure(image = imgtk)\n",
    "        root.update()\n",
    "        lmain.after(10,show_subject)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        exit()\n",
    "\n",
    "#  Function to display the emoji after detecting the emotion of the face\n",
    "        \n",
    "def show_avatar():\n",
    "    frame2 = cv2.imread(emoji_dist[show_text[0]])\n",
    "    pic2 = cv2.cvtColor(frame2,cv2.COLOR_BGR2RGB)\n",
    "    img2 = Image.fromarray(frame2)\n",
    "    imgtk2 = ImageTk.PhotoImage(image=img2)\n",
    "    lmain2.imgtk2 = imgtk2\n",
    "    lmain3.configure(text=emotion_dict[show_text[0]],font=('arial',45,'bold'))\n",
    "    \n",
    "    lmain2.configure(image = imgtk2)\n",
    "    root.update()\n",
    "    lmain2.after(10,show_avatar())\n",
    "               \n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    frame_number = 0\n",
    "    root=tk.Tk()   \n",
    "    lmain = tk.Label(master=root,padx=50,bd=10)\n",
    "    lmain2 = tk.Label(master=root,bd=10)\n",
    "    lmain3=tk.Label(master=root,bd=10,fg=\"#CDCDCD\",bg='black')\n",
    "    lmain.pack(side=LEFT)\n",
    "    lmain.place(x=50,y=250)\n",
    "    lmain3.pack()\n",
    "    lmain3.place(x=960,y=250)\n",
    "    lmain2.pack(side=RIGHT)\n",
    "    lmain2.place(x=900,y=350)\n",
    "    \n",
    "    root.title(\"Photo To Emoji\")            \n",
    "    root.geometry(\"1400x900+100+10\") \n",
    "    root['bg']='black'\n",
    "    exitButton = Button(root, text='Quit',fg=\"red\",command=root.destroy,font=('arial',25,'bold')).pack(side = BOTTOM)\n",
    "    threading.Thread(target = show_subject()).start()\n",
    "    threading.Thread(target = show_avatar()).start()\n",
    "    root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
